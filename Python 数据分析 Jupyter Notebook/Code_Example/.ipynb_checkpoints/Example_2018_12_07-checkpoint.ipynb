{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets 模块常用数据集加载函数及其解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#          数据集加载函数                     数据集任务类型\n",
    "\n",
    "#          load_boston                        回归\n",
    "\n",
    "#          fetch_california_housing           回归\n",
    "\n",
    "#          load_digits                        分类\n",
    "\n",
    "#          load_breast_cancer                 分类、聚类\n",
    "\n",
    "#          load_iris                          分类、聚类\n",
    "\n",
    "#          load_wine                          分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载 breast_cancer 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast_cancer 数据集的长度为： 5\n",
      "breast_cancer 数据集的类型为： <class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer() # 将数据集赋值给 cancer 变量\n",
    "print(\"breast_cancer 数据集的长度为：\", len(cancer))\n",
    "print(\"breast_cancer 数据集的类型为：\", type(cancer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn 自带数据集内部信息获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast_cancer 数据集的数据为：\n",
      " [[1.799e+01 1.038e+01 1.228e+02 ... 2.654e-01 4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
      " ...\n",
      " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
      " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "cancer_data = cancer[\"data\"]\n",
    "print(\"breast_cancer 数据集的数据为：\\n\", cancer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast_cancer 数据仅的标签为：\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "cancer_target = cancer[\"target\"] # 取出数据集的标签\n",
    "print(\"breast_cancer 数据仅的标签为：\\n\", cancer_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast_cancer 数据集的特征名为：\n",
      " ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "cancer_names = cancer[\"feature_names\"] # 取出数据集的特征名\n",
    "print(\"breast_cancer 数据集的特征名为：\\n\", cancer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast_cancer 数据集的描述信息为：\n",
      " Breast Cancer Wisconsin (Diagnostic) Database\n",
      "=============================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      "References\n",
      "----------\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancer_desc = cancer[\"DESCR\"] # 取出数据集的描述信息\n",
    "print(\"breast_cancer 数据集的描述信息为：\\n\", cancer_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_test_split 常用参数及其说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split 函数：\n",
    "\n",
    "# sklearn.model_selection.train_test_split(*arrays, **options)\n",
    "\n",
    "# *arrays          接收一个或多个数据集。代表需要划分的数据集。若为分类回归，则分别传入数据和标签；若为聚类，则传入数据。无默认\n",
    "\n",
    "# test_size        接收 float、int 类型的数据或者 None。代表测试集的大小。如果传入的为 float 类型的数据，则需要限定在 0~1 之间，\n",
    "#                  代表测试集在总数中的占比；如果传入的为 int 类型的数据，则表示测试集记录的绝对数目。该参数与 train_size 可以\n",
    "#                  只传入一个。在 0.21 版本前，若 test_size 和 train_size 均为默认，则 test_size 为 25%\n",
    "\n",
    "# train_size       接收 float、int 类型的数据或者 None。代表训练集的大小。该参数与 test_size 可以只传入一个\n",
    "\n",
    "# random_state     接收 int。代表随机种子编号，相同随机种子编号产生相同的随机结果，不同的随机种子编号产生不同的随机结果。默认为 None\n",
    "\n",
    "# shuffle          接收 boolean。代表是否进行有放回抽样。若该参数取值为 True，则 stratify 参数必须不能为空\n",
    "\n",
    "# stratify         接收 array 或者 None。如果不为 None，则使用传入的标签进行分层抽样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 train_test_split 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据集数据的形状为： (569, 30)\n",
      "原始数据集标签的形状为： (569,)\n"
     ]
    }
   ],
   "source": [
    "print(\"原始数据集数据的形状为：\", cancer_data.shape)\n",
    "print(\"原始数据集标签的形状为：\", cancer_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据的形状为： (455, 30)\n",
      "训练集标签的形状为： (455,)\n",
      "测试集数据的形状为： (114, 30)\n",
      "测试集标签的形状为： (114,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer_data_train, cancer_data_test, cancer_target_train, cancer_target_test = \\\n",
    "    train_test_split(cancer_data, cancer_target, test_size = 0.2, random_state = 42)\n",
    "print(\"训练集数据的形状为：\", cancer_data_train.shape)\n",
    "print(\"训练集标签的形状为：\", cancer_target_train.shape)\n",
    "print(\"测试集数据的形状为：\", cancer_data_test.shape)\n",
    "print(\"测试集标签的形状为：\", cancer_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 转换器的 3 个方法及其说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit                fit 方法主要通过分析特征和目标值提取有价值的信息，这些信息可以是统计量，也可以是权值系数等\n",
    "\n",
    "# transform          transform 方法主要用来对特征进行转换。从可利用信息的角度分为无信息转换和有信息转换。无信息转换是指不利用\n",
    "#                    任何其他信息进行转换，比如指数和对数函数转换等。有信息转换根据是否利用目标值向量又可分为无监督转换和有监\n",
    "#                    督转换。无监督转换指只利用特征的统计信息的转换，比如标准化和 PCA 降维等。有监督转换指既利用了特征信息又利\n",
    "#                    用了目标值信息的转换，比如通过模型选择特征和 LDA 降维等\n",
    "\n",
    "# fit_transform      fit_transform 方法就是先调用 fit 方法，然后调用 transform 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 离差标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "离差标准化前训练集数据的最小值为： 0.0\n",
      "离差标准化后训练集数据的最小值为： 0.0\n",
      "离差标准化前训练集数据的最大值为： 4254.0\n",
      "离差标准化后训练集数据的最大值为： 1.0000000000000002\n",
      "离差标准化前测试集数据的最小值为： 0.0\n",
      "离差标准化后测试集数据的最小值为： -0.057127602776294695\n",
      "离差标准化前测试集数据的最大值为： 3432.0\n",
      "离差标准化后测试集数据的最大值为： 1.3264399566986453\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "Scaler = MinMaxScaler().fit(cancer_data_train) # 生成规则\n",
    "\n",
    "# 将规则应用于训练集\n",
    "\n",
    "cancer_trainScaler = Scaler.transform(cancer_data_train)\n",
    "\n",
    "# 将规则应用于测试集\n",
    "\n",
    "cancer_testScaler = Scaler.transform(cancer_data_test)\n",
    "\n",
    "print(\"离差标准化前训练集数据的最小值为：\", np.min(cancer_data_train))\n",
    "print(\"离差标准化后训练集数据的最小值为：\", np.min(cancer_trainScaler))\n",
    "print(\"离差标准化前训练集数据的最大值为：\", np.max(cancer_data_train))\n",
    "print(\"离差标准化后训练集数据的最大值为：\", np.max(cancer_trainScaler))\n",
    "print(\"离差标准化前测试集数据的最小值为：\", np.min(cancer_data_test))\n",
    "print(\"离差标准化后测试集数据的最小值为：\", np.min(cancer_testScaler))\n",
    "print(\"离差标准化前测试集数据的最大值为：\", np.max(cancer_data_test))\n",
    "print(\"离差标准化后测试集数据的最大值为：\", np.max(cancer_testScaler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn 部分预处理函数与其作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler           对特征进行标准差标准化\n",
    "\n",
    "# Normalizer               对特征进行归一化\n",
    "\n",
    "# Binarizer                对定量特征进行二值化处理\n",
    "\n",
    "# OneHotEncoder            对定性特征进行独热编码处理\n",
    "\n",
    "# FunctionTransformer      对特征进行自定义函数变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对 breast_cancer 数据集 PCA 降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 降维前训练集数据的形状为： (455, 30)\n",
      "PCA 降维后训练集数据的形状为： (455, 10)\n",
      "PCA 降维前测试集数据的形状为： (114, 30)\n",
      "PCA 降维后测试集数据的形状为： (114, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_model = PCA(n_components = 10).fit(cancer_trainScaler) # 生成规则\n",
    "\n",
    "# 将规则应用于训练集\n",
    "\n",
    "cancer_trainPca = pca_model.transform(cancer_trainScaler)\n",
    "\n",
    "# 将规则应用于测试集\n",
    "\n",
    "cancer_testPca = pca_model.transform(cancer_testScaler)\n",
    "\n",
    "print(\"PCA 降维前训练集数据的形状为：\", cancer_trainScaler.shape)\n",
    "print(\"PCA 降维后训练集数据的形状为：\", cancer_trainPca.shape)\n",
    "print(\"PCA 降维前测试集数据的形状为：\", cancer_testScaler.shape)\n",
    "print(\"PCA 降维后测试集数据的形状为：\", cancer_testPca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA 降维算法函数常用参数及其作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components          接收 None、int、float 或 mle。未指定时，代表所有特征均会被保留下来；如果为 int，则表示将原始数据降低到 n 个维度；\n",
    "#                       如果为 float，则 PCA 根据样本特征方差来决定降维后的维度数；赋值为 \"mle\"，PCA 会用 MLE 算法根据特征的方差分布情况\n",
    "#                       自动选择一定数量的主成分特征来降维。默认为 None\n",
    "\n",
    "# copy                  接收 boolean。代表是否在运行算法时降原始数据复制一份，如果为 True，则运行后，原始数据的值不会有任何改变；如果为\n",
    "#                       False，则运行 PCA 算法后，原始数据的值会发生改变。默认为 True\n",
    "\n",
    "# whiten                接收 boolean。表示白化。所谓白化，就是对降维后的数据的每个特征进行归一化，让方差都为 1。默认为 False\n",
    "\n",
    "# svd_solver            接收 auto、full、arpack、randomized。代表使用的 SVD 算法。randomized 一般适用于数据量大，数据维度多，同时主成分数\n",
    "#                       目比例又较低的 PCA 降维，它使用了一些加快 SVD 的随机算法。full 是使用 SciPy 库实现的传统 SVD 算法。arpack 和\n",
    "#                       randomized 的适用场景类似，区别是，randomized 使用的是 sklearn 自己的 SVD 实现，而 arpack 直接使用了 SciPy 库的\n",
    "#                       sparse SVD 实现。auto 则代表 PCA 类会自动在上述 3 种算法中去权衡，选择一个合适的 SVD 算法来降维。默认为 auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取 sklearn 自带的 boston 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boston 数据集数据的形状为： (506, 13)\n",
      "boston 数据集标签的形状为： (506,)\n",
      "boston 数据集特征名的形状为： (13,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "boston_data = boston[\"data\"]\n",
    "boston_target = boston[\"target\"]\n",
    "boston_names = boston[\"feature_names\"]\n",
    "\n",
    "print(\"boston 数据集数据的形状为：\", boston_data.shape)\n",
    "print(\"boston 数据集标签的形状为：\", boston_target.shape)\n",
    "print(\"boston 数据集特征名的形状为：\", boston_names.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 train_test_split 划分 boston 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据的形状为： (404, 13)\n",
      "训练集标签的形状为： (404,)\n",
      "测试集数据的形状为： (102, 13)\n",
      "测试集标签的形状为： (102,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston_data_train, boston_data_test, boston_target_train, boston_target_test = \\\n",
    "    train_test_split(boston_data, boston_target, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(\"训练集数据的形状为：\", boston_data_train.shape)\n",
    "print(\"训练集标签的形状为：\", boston_target_train.shape)\n",
    "print(\"测试集数据的形状为：\", boston_data_test.shape)\n",
    "print(\"测试集标签的形状为：\", boston_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 stdScale.transform 进行数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准差标准化后训练集数据的方差为： 1.0000000000000002\n",
      "标准差标准化后训练集数据的均值为： 1.3589873916051421e-15\n",
      "标准差标准化后测试集数据的方差为： 0.94753649080047\n",
      "标准差标准化后测试集数据的均值为： 0.03075300289370519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdScale = StandardScaler().fit(boston_data_train) # 生成规则\n",
    "\n",
    "# 将规则应用于训练集\n",
    "\n",
    "boston_trainScaler = stdScale.transform(boston_data_train)\n",
    "\n",
    "# 将规则应用于测试集\n",
    "\n",
    "boston_testScaler = stdScale.transform(boston_data_test)\n",
    "\n",
    "print(\"标准差标准化后训练集数据的方差为：\", np.var(boston_trainScaler))\n",
    "print(\"标准差标准化后训练集数据的均值为：\", np.mean(boston_trainScaler))\n",
    "print(\"标准差标准化后测试集数据的方差为：\", np.var(boston_testScaler))\n",
    "print(\"标准差标准化后测试集数据的均值为：\", np.mean(boston_testScaler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 pca.transform 进行 PCA 降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "降维后 boston 数据集数据测试集的形状为： (404, 5)\n",
      "降维后 boston 数据集数据训练集的形状为： (102, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 生成规则\n",
    "\n",
    "pca = PCA(n_components = 5).fit(boston_trainScaler)\n",
    "\n",
    "# 将规则应用于训练集\n",
    "\n",
    "boston_trainPca = pca.transform(boston_trainScaler)\n",
    "\n",
    "# 将规则应用于测试集\n",
    "\n",
    "boston_testPca = pca.transform(boston_testScaler)\n",
    "\n",
    "print(\"降维后 boston 数据集数据测试集的形状为：\", boston_trainPca.shape)\n",
    "print(\"降维后 boston 数据集数据训练集的形状为：\", boston_testPca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
